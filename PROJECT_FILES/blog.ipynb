{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Text Analytics Splunk App - Blog\n",
    "##### Author: Nathan Worsham\n",
    "##### Created for MSDS692 Data Science Practicum I at Regis University, 2018\n",
    "\n",
    "Most other projects for the Data Science Practicum revolved around collecting a dataset, cleaning it, exploring it, and then using machine learning algorithms on it to perform some sort of prediction or classification. I decided to go a bit outside the box and instead build a Splunk App for my project.\n",
    "\n",
    "## Inspiration\n",
    "Let me first preface by stating I am not a developer... but I don't let that stop me. Like many of my cohorts, I have always worked in information technology but I have a completely unrelated background--music. I tinker, I experiment, I'm curious, I push and push until I find an answer. I don't always know the correct or proper methodolgies but I am willing to learn. I have however built a few Splunk Apps previously (using the same techniques just mentioned) so I was at least familiar with the practice and just before this class, I took a class on text analytics. I am fascinated with Splunk, Python, and the subject of text analytics. I already had a few (internal) dashboards taking a shot at text analytics if only mildly. After the course though, I realized some of the shortcomings that Splunk had regarding text analytics. Granted system logs are not something you think of immediately having natural language (some actually do) but that is not the only source of text in Splunk as it can consume anything human readable. In my own place of work, one immediate example that stood out in my mind was support ticket summaries and descriptions but I'm sure the sources do not stop there. \n",
    "\n",
    "## Description\n",
    "So given that information, the intent of this project is create a Splunk app that will provide a simple interface for analyzing any text a user points at it in Splunk using Python natural language processing libraries (currently just NLTK 3.3). The app provides custom commands and dashboards to show how to use it and walk a user through text analytics.\n",
    "\n",
    "## The Details\n",
    "This project represents exisiting knowledge I already have in Splunk (regarding developing apps, administration, using SPL, building dashboards and I'm sure more) and building upon it. Splunk apps that want to use new Python libraries generally get packaged with those additional libraries needed to run the custom commands within the app, doing so by importing locally--when you say `import` in Python, the first place it looks is in the current working directory. This of course makes sense as Splunk doesn't want to break their existing Python environment by allowing admins to add to and change it permanently. Splunk also supplies a Python SDK (http://dev.splunk.com/python) which takes care of many remedial tasks and keeps things consistent. I will be relying on this heavily and using the documenting standards they give in their examples within the SDK which are a bit different then say sphinx markup or at least I think so. \n",
    "\n",
    "#### Getting NLTK Working\n",
    "\n",
    "As far as Python libraries go for natural language processing there are really two major contenders spaCy (https://spacy.io) and nltk or _natural language toolkit_ (http://www.nltk.org). I really like the features and performance of spaCy but during this project I found one major downfall for spaCy. Since it is is a C extension--simple way to tell is if the package extension is `.so`--it would have to be compiled for the host system first before it can be imported locally. Is this an impossible step to overcome? No, I'm sure it is not. However this practicum class is only 8 weeks long, so time is of the essense. Relatedly, early on in the process I decided to go with the concept of MVP or minimum viable product, to get the pieces working as best I could and if there was still time, go back and iterate to make improvements. There is a saying I am always telling my kids... \"Do the best with what you got\"... so it was time to take my own advice.\n",
    "\n",
    "I started my app by making a shell of just the app name and a bin directory--granted Splunk does provide an add-on builder app to help with this but maybe I am just lazy or stubborn as I am yet to use it--and downloaded and placed the nltk directory there. Consequently pypi.org is a great place to manually download Python packages and where I did so (https://pypi.org/project/nltk/). I then changed my directory into the bin folder (I split my development time between a Mac and a Linux server) and ran Splunk's CLI command to get into its Python IDE. The command is `$SPLUNK_HOME/bin/splunk cmd python` (replacing `$SPLUNK_HOME` with the actualy directory if this is not in your environment path), and is great way to test out to see if Splunk will succeed in simply importing the library or has further dependancies. \n",
    "\n",
    "The first problem I ran into is that it could not import six (https://pypi.org/project/six/), but I have needed that package for other apps before, so I copied it over and tried again. I then received an error of wanting to import sqlite3. This was a bit more confusing as allegedly this is supposed to be packaged in Python since 2.5 which was captured nicely in a stackoverflow (2013) answer https://stackoverflow.com/questions/19530974/how-can-i-install-sqlite3-to-python, Splunk’s mini python environment is currently 2.7.14. However the very next answer from the same post was truly what I needed, which showed me I could download pysqlite to satisfy the condition. However once downloading the files, it wasn’t evident which were the right files, I finally figured out to just grab the folder called “lib” into my app’s bin directory and rename it to sqllite3. Now importing nltk works. \n",
    "\n",
    "The next problem for packaging nltk however is that nltk expects certain documents to be downloaded and available. This includes corpora, tagsets, tokenizers and more. In my case though I would want to control what is downloaded and where it is downloaded to. A great Medium article (satoru, 2016) pointed me to the place where all of this data could be downloaded which is http://www.nltk.org/nltk_data/. Though ultimately a stackoverflow.com (2018) answer showed that I could either append the value nltk.data.path or set an environment variable. Once I appended the value and downloaded the data I was able to import stop words successfully. So my custom command would need to add to that path every time.\n",
    "\n",
    "#### Custom Command - cleantext\n",
    "\n",
    "First order of business was creating a new custom command to package with the app for cleaning text. Now I knew that Splunk natively could certainly handle pieces of this already--like lowercase, split words by spaces, remove digits and punctuation--but having a single command to complete this is compelling and useful but more importantly Splunk does not have a native way to complete word stemming or lemmatization. Stemming and lemmatization are all about changing words back to their base. Personally I don't really like stemming as often the result is not a real word at all as often it just chops off suffixes but if I am already providing lemmatization maybe someone else has a good reason for it--plus stemming is very fast comparatively. So my command would provide the cleaning options I mentioned earlier, changing words back to their base, and also an option to remove URLs because when you remove the punctuation out of a URL you are left with jibberish. \n",
    "\n",
    "The first part of creating a custom command with Splunk's SDK that I would deal with was options. Referring specically to what options the command would take.  I learned that the SDK’s options are quite involved, it has validators that can validate if the option the user gave is any of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        Boolean\n",
    "        Code\n",
    "        Duration\n",
    "        File\n",
    "        Integer\n",
    "        List\n",
    "        Map\n",
    "        RegularExpression\n",
    "        Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also learned I could set a default for the option with simply providing `default=<myvalue>` and even require the option with `require=True`. Putting it all together, an example option looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    default_clean = Option(\n",
    "        default=True,\n",
    "        doc='''**Syntax:** **lowercase=***<boolean>*\n",
    "        **Description:** Change text to lowercase, remove punctuation, and removed numbers, defaults to true''',\n",
    "        validate=validators.Boolean()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validators for boolean are great in Splunk, because it will take lots of different options to equal true or false like t or true or TRUE or 1 or True. \n",
    "\n",
    "I started with just making my command simple and changing text to lowercase by default, I successfully changed it to lowercase based on the options given at the Splunk search input. \n",
    "\n",
    "I made a decision that it would be more efficient to lowercase, remove punctuation and numbers all at the same time and that these could be treated as one option (on or off) given that this is something you can already do in Splunk so it is more of a convenience factor more than anything though here I struggled with what to call the combined option: \n",
    "\n",
    "std_clean <br/>\n",
    "basic_wash <br/>\n",
    "basic_clean <br/>\n",
    "std_options <br/>\n",
    "basic_option_group <br/>\n",
    "default_clean\n",
    "\n",
    "I settled on the last… default_clean (obviously since that is the example I gave). Actually writing out the possible options was a good exercise, this goes back to the old adage by Phil Karlton “There are only two hard things in Computer Science: cache invalidation and naming things.” (Hilton, 2016), which is so true but also can be pretty important and sometimes difficult to change. I ran into similar difficulty for the other options I needed to name as well and the command ended up with A LOT of options which is especially painful when it comes to documenting the command--something I find that often takes longer to write than the code itself.\n",
    "\n",
    "I would say throughout the creation of the cleantext command I played tug-of-war with the following:\n",
    "* order of operations in general\n",
    "* DRY (Do Not Repeat Yourself) programming versus providing user options\n",
    "* speed of the command (`set` by the way is awesome, turn a list into a set and Python does not have to look at each element)\n",
    "\n",
    "Anyway these can have competing interests. During the creation of the command I realized very quick that I would need some quick text to test on (other than using `makeresults` and an `eval`). I had planned on packaging the app with some texts from the Gutenburg library anyway, so I paused to clean and reformat several texts--Peter Pan, Pride and Prejudice, and Moby Dick--the jupyter notebooks are in this same project folder. I made the texts into CSV files, broken up by sentence but also including a title, author, and chapter columns. This way I could easily use Splunk's `inputlookup` command to import them and I used the example text to test out speed. Splunk has an “Inspect Job” feature that tells the overall time but the times can vary based on the business of the computer so I do a couple of takes per trial.\n",
    "\n",
    "Some other issues I ran into during the creation of this command is parts-of-speech (POS) tagging and its relation to lemmatization. POS tagging is labeling words in their grammatical structure for words such as noun, verb, and so on. NLTK's lemmatization command (`WordNetLemmatizer`) assumes that all words are nouns if you do not feed it the POS tag. The odd part is that the POS tagger nltk provides does not output the tag in a form that the lemmatizer accepts so you have to convert it. Splunk is good about spitting out Python errors to the screen when a failure happens but this isn't always clear what exactly is the cause of the issue... enter logging. Splunk fortunately provides developers lots of references and tools to make them successful and you would expect a log aggregation tool be able to log itself. Sure enough I used their implementation (http://dev.splunk.com/view/logging/SP-CAAAFCN) to help me get around problem spots. Finally I had some issues with tokenizing the words. Here I found that nltk’s word tokenizer does a nice job of breaking the words into a list (which shows up as a multi-valued field in Splunk which was the ideal output). Just splitting on spaces but in the example text would sometimes result in two words on a single space because they were separated by punctuation. To get POS tags I needed to use the word tokenizer but it is certainly slower than splitting by space. I found a new method of Python's regex command--`re.split()`--which does really well with `\\W+` as the splitter but then it ends up removing punctuation, so I used that when the user chose not to do lemmatization based on POS.  \n",
    "\n",
    "#### EDA - Counts Dashboard\n",
    "\n",
    "This dashboard of the app would fulfill the exporatory data analysis portion of the class rubric and useful for answering some basic questions about a corpus. Here I made good use of Splunk's ability to use base searches behind the scenes to power multiple panels of the dashboard. Honestly I am of the belief that dashboards are the most powerful aspect of Splunk and possibly the least used, this is because you essentially can create very complex \"code\" (okay it is really SPL and XML underneath but really SPL is a coding language of its own accord) and then that code can be called again easily and quickly. In fact even for those very experienced in SPL too can benefit from the dashboard because you can run multiple searches at once and--back to the complex aspect--not have to remember how you did something before. Anyway a base search can pull back a large dataset and the panels can make small adjustments to portions of said dataset to visualize it. \n",
    "\n",
    "The concept of the dashboard is just as the name implies, tokenize the words in the corpus and get counts. Of course counts also includes total counts, counts by parts-of-speech tags, a word cloud, as well as some drill down options and zipf plot if the user is including stopwords.\n",
    "\n",
    "Running the dashboard on one of the sample texts I included with the app--Moby Dick--we see unsurprisingly that \"whale\" is the top word:\n",
    "![moby_dick_counts](img/moby_dick_counts.png)\n",
    "\n",
    "Clicking on the POS tag NN, we get an explanation of singular nouns from Dipanjan Sarkar's book _Text Analytics with Python: A Practical Real-World Approach to Gaining Actionable Insights from your Data._ (2016) and see that 686 of the 1299 whale mentions where used as a singular noun.\n",
    "![moby_dick_NN](img/moby_dick_NN.png)\n",
    "\n",
    "Looking at six months of support tickets and removing some custom stopwords (this is because many of these tickets come through as emails so the signatures cause a lot of unneccessary repeating words).\n",
    "![support_tickets_counts_NN](img/support_tickets_counts_NN.png)\n",
    "\n",
    "Nice to see that people are still very polite with \"please\" being the top word. However \"please\" gets incorrectly intrepreted often as a noun. One factor that might cause this is that when users create support tickets they do not necessarily follow grammatical correctness, it often times is more similar to a text message than to an actual email. Though also given that noun is the default POS tag, I would think anything the tagger cannot interpret ends up a noun. \n",
    "\n",
    "Look at the word cloud set to the full 400 terms a couple of things that stand out to me is users are needing help with email, passwords, and directory access.\n",
    "![support_tickets_wordcloud](img/support_tickets_wordcloud.png)\n",
    "\n",
    "Looking at the support tickets with stopwords left in tact, interestingly we see that \"to\" is the top word. Normally \"the\" is the most common word, but this might again go back to the language norms surrounding opening support tickets. Though Splunk does not currently have a visualization that supports both logrithmic scales and points with a line through it, so I could not point the distribution line with it. However without the stopwords the tickets do appear to follow the Zipf distribution as expected.\n",
    "![support_tickets_zipf](img/support_tickets_zipf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
