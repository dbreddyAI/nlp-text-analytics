<form hideEdit="true">
  <label>Welcome</label>
  <fieldset submitButton="false"></fieldset>
  <row>
    <panel>
      <title>NLP Terminology</title>
      <input id="long_link_input" type="link" token="nlp">
        <label></label>
        <fieldForLabel>Terminology</fieldForLabel>
        <fieldForValue>Terminology</fieldForValue>
        <search>
          <query>| inputlookup nlp_terminology.csv</query>
        </search>
        <default>NLP</default>
        <initialValue>NLP</initialValue>
      </input>
      <single>
        <search>
          <query>| inputlookup nlp_terminology.csv 
| where Terminology=="$nlp$"
| table Explanation Link</query>
          <earliest>-24h@h</earliest>
          <latest>now</latest>
        </search>
        <option name="colorMode">block</option>
        <option name="drilldown">all</option>
        <option name="height">56</option>
        <option name="rangeColors">["0x53a051","0x0877a6","0xf8be34","0xf1813f","0xdc4e41"]</option>
        <option name="refresh.display">progressbar</option>
        <option name="useColors">1</option>
        <drilldown>
          <link target="_blank">https://en.wikipedia.org/wiki/$row.Link$</link>
        </drilldown>
      </single>
    </panel>
  </row>
  <row>
    <panel>
      <html>
        <div>
          The intent of this app is to provide a simple interface for analyzing text in Splunk using python natural language processing libraries (currently just NLTK 3.3). The app provides custom commands (currently 3), additional ML algorithms (currently 3) to use with Splunk's ML Toolkit, and dashboards to show how to use. The app is also packaged with Gutenberg texts (currenlty 3) formatted as CSV lookups. 
        </div>
        <h1>Requirements</h1>
        <div>
<a href="https://splunkbase.splunk.com/app/2890/">Splunk ML Toolkit 3.2 or greater</a>
          <br/>
<a href="https://splunkbase.splunk.com/app/3212/">Wordcloud Custom Visualization</a>
          <br/>
<a href="https://splunkbase.splunk.com/app/3137/">Parallel Coordinates Custom Visualization</a>
          <br/>
<a href="https://splunkbase.splunk.com/app/3767/">Force Directed App For Splunk</a>
          <br/>
        </div>
        <h1>Custom Commands</h1>
        <h2>bs4</h2>
        <div> A wrapper for BeautifulSoup4 to extract html/xml tags and text from them to use in Splunk. A wrapper script to bring some functionality from BeautifulSoup to Splunk. Default is to get the text and send it to a new field 'get\_text', otherwise the selection is returned in a field named 'soup'. Default is to use the 'lxml' parser, though you can specify others, 'html5lib' is not currently included. The find methods can be used in conjuction, their order of operation is find &gt; find\_all &gt; find\_child &gt; find children. Each option has a similar named option appended '\_attrs' that will accept inner and outer quoted key:value pairs for more precise selections.
        </div>
        <h3>Syntax</h3>
        <pre>* | bs4 textfield=&lt;field&gt; [get_text=&lt;bool&gt;] [get_text_label=&lt;string&gt;] [parser=&lt;string&gt;] [find=&lt;tag&gt;] [find_attrs=&lt;quoted_key:value_pairs&gt;] [find_all=&lt;tag&gt;] [find_all_attrs=&lt;quoted_key:value_pairs&gt;] [find_child=&lt;tag&gt;] [find_child_attrs=&lt;quoted_key:value_pairs&gt;] [find_children=&lt;tag&gt;] [find_children_attrs=&lt;quoted_key:value_pairs&gt;]</pre>
        <h2>cleantext</h2>
        <div>Tokenize and normalize text (remove punctuation, digits, change to base_word). Different options result in better and slower cleaning. base_type="lemma_pos" being the slowest option, base_type="lemma" assumes every word is a noun, which is faster but still results in decent lemmatization. Many fields have a default already set, textfield is only required field. By default results in a multi-valued field which is ready for used with stats count by. Optionally return special fields for analysis--pos_tags and ngrams.
        </div>
        <h3>Syntax</h3>
        <pre>* | cleantext textfield=&lt;field&gt; [default_clean=&lt;bool&gt;] [remove_urls=&lt;bool&gt;] [remove_stopwords=&lt;bool&gt;] [base_word=&lt;bool&gt;] [base_type=&lt;string&gt;] [mv=&lt;bool&gt;] [force_nltk_tokenize=&lt;bool&gt;] [pos_tagset=&lt;string&gt;] [custom_stopwords=&lt;comma_separated_string_list&gt;] [term_min_len=&lt;int&gt;] [ngram_range=&lt;int&gt;-&lt;int&gt;] [ngram_mix=&lt;bool&gt;]
	</pre>
        <h2>vader</h2>
        <div>Sentiment analysis using Valence Aware Dictionary and sEntiment Reasoner. Using option full_output will return scores for neutral, positive, and negative which are the scores that make up the compound score (that is just returned as the field "sentiment". Best to feed in uncleaned data as it takes into account capitalization and punctuation.
        </div>
        <h3>Syntax</h3>
        <pre>* | vader textfield=&lt;field&gt; [full_output=&lt;bool&gt;]</pre>
        <h1>ML Algorithms</h1>
        <h2>TruncantedSVD</h2>
        <div>From sklearn. Used for dimension reduction (especially on a TFIDF). This is also known in text analytics as Latent Semantic Analysis or LSA. Returns fields prepended with "SVD_". See <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html">http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html</a>
        </div>
        <h3>Syntax</h3>
        <pre>* | fit TruncantedSVD &lt;fields&gt; [into &lt;model name&gt;] k=&lt;int&gt;</pre>
        <h2>LatentDirichletAllocation</h2>
        <div>From sklearn. Used for dimension reduction. This is also known as LDA. Returns fields prepended with "LDA_". See <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html">http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html</a>
        </div>
        <h3>Syntax</h3>
        <pre>* | fit LatentDirichletAllocation &lt;fields&gt; [into &lt;model name&gt;] k=&lt;int&gt;</pre>
        <h2>NMF</h2>
        <div>From sklearn. Used for dimension reduction. This is also known as Non-Negative Matrix Factorization. Returns fields prepended with "NMF_". See <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html">http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html</a>
        </div>
        <h3>Syntax</h3>
        <pre>* | fit NFM &lt;fields&gt; [into &lt;model name&gt;] [k=&lt;int&gt;]</pre>
        <h1>Known Issues</h1>
	<div>Version 7.0.0 introduced an issue that causes errors in the ML Toolkit when using free or developer's license see <a href="https://answers.splunk.com/answers/654411/splunk-710-upgrade-of-free-version-finalizes-searc.html">https://answers.splunk.com/answers/654411/splunk-710-upgrade-of-free-version-finalizes-searc.html</a>. Has not been fixed as of 7.1.1.</div>
<div>Splunk SDK crashes when too much data is sent through it, gets a buffer error. See <a href="https://github.com/splunk/splunk-sdk-python/issues/150">https://github.com/splunk/splunk-sdk-python/issues/150</a>. Workaround would be to used the sample command to down sample the data until it works.</div>
        <div>
Topic Modeling Algorithms currently cannot be saved into models (using the "into" argument).
        </div>
      </html>
    </panel>
  </row>
</form>
