[bs4-command]
syntax = bs4 textfield=<field> (get_text=<bool>)? (get_text_label=<string>)? (get_attr=<attribute_name_string>)? (parser=<parser-options>)? (find=<tag>)? (find_attrs=<quoted_key:value_pairs>)? (find_all=<tag>)? (find_all_attrs=<quoted_key:value_pairs>)? (find_child=<tag>)? (find_child_attrs=<quoted_key:value_pairs>)? (find_children=<tag>)? (find_children_attrs=<quoted_key:value_pairs>)?
shortdesc = A wrapper for BeautifulSoup4 to extract html/xml tags and text from to use in Splunk.
description =  A wrapper script to bring some functionality from BeautifulSoup to Splunk. \
    Default is to get the text and send it to a new field 'get_text', otherwise the selection \
    is returned in a field named 'soup'. Default is to use the 'lxml' parser, though you can \
    specify others, 'html5lib' is not currently included. The find methods can be used in \
    conjuction, their order of operation is find > find_all > find_child > find children. \
    Each option has a similar named option appended '_attrs' that will accept inner and outer \
    quoted key:value pairs for more precise selections.
usage = public
example1 = * | bs4 textfield=_raw find="div" get_text=t

[cleantext-command]
syntax = cleantext textfield=<field> (keep_orig=<bool>)? (default_clean=<bool>)? (remove_urls=<bool>)? (remove_stopwords=<bool>)? (base_word=<bool>)? (base_type=<base_type-options>)? (mv=<bool>)? (force_nltk_tokenize=<bool>)? (pos_tagset=<pos_tagset-options>)? (custom_stopwords=<comma_separated_string_list>)? (term_min_len=<int>)? (ngram_range=<int>-<int>)? (ngram_mix=<bool>)?
shortdesc = Tokenize and normalize text (remove punctuation, digits, change to base_word)
description = Tokenize and normalize text (remove punctuation, digits, change to base_word) \
    Different options result in better and slower cleaning. base_type="lemma_pos" being the \
    slowest option, base_type="lemma" assumes every word is a noun, which is faster but still \
    results in decent lemmatization. Many fields have a default already set, textfield is only \
    required field. By default results in a multi-valued field which is ready for used with \
    stats count by. Optionally return special fields for analysis--pos_tags and ngrams.
usage = public
example1 = * | cleantext textfield=sentence

[vader-command]
syntax = vader textfield=<field> (full_output=<bool>)?
shortdesc = Sentiment analysis using Valence Aware Dictionary and sEntiment Reasoner
description = Sentiment analysis using Valence Aware Dictionary and sEntiment Reasoner \
    Using option full_output will return scores for neutral, positive, and negative which \
    are the scores that make up the compound score (that is just returned as the field \
    "sentiment". Best to feed in uncleaned data as it takes into account capitalization \
    and punctuation.
usage = public
example1 = * | vader textfield=sentence

[parser-options]
syntax = html.parser|lxml|lxml-xml|xml

[base_type-options]
syntax = lemma|lemma_pos|stem

[pos_tagset-options]
syntax = None|universal
